#!/usr/bin/env python3
__author__ = "Jan Balewski"
__email__ = "janstar1122@gmail.com"

'''
Jan:
give my python code using only numpy and COBYLA which reads in iris data set, constructs ML classifier consisting of 3 layers of fully connected network  4-5-3. The last layer should represent the 3 classes of iris hot encoded. COBYLA should be used as minimizer
Use cross-entropy loss
add bias terms to fully connected layers
Print loss vs. iteration at the end

Note, a single node hidden layer: 4-1-3, which has only 11 trainable params, I get 97% accuracy.

Code below is moslty generated by ChatGPT4

Iteration 800: Loss = 0.060
num par= (19,) W in [-4.68,6.71] avr=0.69
Accuracy: 100.00%

confusion matrix, test samples:30
true:0  reco:[10  0  0]
true:1  reco: [0  9  0]
true:2  reco:[ 0  0 11]


'''

import numpy as np
from scipy.optimize import minimize
from scipy.optimize import basinhopping
from scipy.optimize import differential_evolution
from sklearn import datasets
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Cross-entropy loss function
#...!...!....................
def cross_entropy_loss(Y_pred, Y_true):
    #print('CNE',Y_pred.shape, Y_true.shape); one_hot # CNE (120, 3) (120, 3)
    m = Y_true.shape[0]
    loss = -np.sum(Y_true * np.log(Y_pred + 1e-9)) / m  # Adding a small value to prevent log(0)
    return loss

# Loss function for optimization including biases
#...!...!....................
def loss_function(params, X, Y):    
    Y_pred = forward_pass(X, params)
    loss = cross_entropy_loss(Y_pred, Y)
    loss_history.append(loss)
    return loss

# Activation functions
#...!...!....................
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=1, keepdims=True)

# Forward pass including biases
#...!...!....................
def forward_pass(X, params):
    # Unpack parameters
    W1 = params[:end_W1].reshape(input_size, hidden_size)
    b1 = params[end_W1:end_b1]
    W2 = params[end_b1:end_W2].reshape(hidden_size, output_size)
    b2 = params[end_W2:]

    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    return A2

#...!...!....................
def normalize_input_data(X):
    # The standard score of a sample x is calculated as:  z = (x - u) / s
    scaler = StandardScaler()
    Z = scaler.fit_transform(X)
    print('NID end: Z shape:',Z.shape)
    
    if 0:
        blowFact=1.
        print('NID add tanh transform, blowFact=',blowFact)
        Z=np.tanh(blowFact*Z)
    if 0:
        blowFact=4
        print('NID add atan transform, blowFact=',blowFact)
        Z=np.arctan(blowFact*Z) *2/np.pi
    if 0: # Z to prob
        print('transform Z-scaling to prob-scaling')
        nSig=1.
        Z=np.clip(Z,-nSig,nSig) # clip values beyond +/- 2 sigma
        Z/=nSig*2  # rescale values to range [0,1]

    print_range(Z,'inp norm')
      
    return Z

#...!...!....................
def print_range(Z,text):
    print('\ndata range (%s)  shape:'%text,Z.shape)
    print('mean :',np.mean(Z,axis=0))
    print('std  :',np.std(Z,axis=0))
    print('min  :',np.min(Z,axis=0))
    print('max  :',np.max(Z,axis=0))


    
#=================================
#=================================
#  M A I N 
#=================================
#=================================
if __name__ == "__main__":
    np.set_printoptions(precision=3)
    # Load the Iris dataset
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target

    # Preprocess the data
    X_scaled =normalize_input_data(X)
    encoder = OneHotEncoder(sparse=False)
    y_onehot = encoder.fit_transform(y.reshape(-1, 1))

     
    # Neural network parameters
    input_size = X.shape[1] # input features (4)
    hidden_size = 1 
    output_size = y_onehot.shape[1]  # number of categories (3)
    mxIter=800  # for COBYLA
    rnd_seed=42 # for reproducibility of data split

    
    # Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.3, random_state=rnd_seed)


    # Initialize weights
    np.random.seed(rnd_seed)
    W1 = np.random.rand(input_size, hidden_size)
    b1 = np.random.rand(hidden_size)
    W2 = np.random.rand(hidden_size, output_size)
    b2 = np.random.rand(output_size)


    # Flatten weights and biases for optimization
    initial_weights = np.hstack((W1.ravel(), b1, W2.ravel(), b2))

    # Define the end indices for each set of parameters
    end_W1 = input_size * hidden_size
    end_b1 = end_W1 + hidden_size
    end_W2 = end_b1 + hidden_size * output_size

    # Loss history
    loss_history = []

    if 1: # Use COBYLA optimizer with set maximum of iterations
        result = minimize(fun=loss_function, 
                      x0=initial_weights, 
                      args=(X_train, y_train), 
                      method='COBYLA', 
                      options={'maxiter': mxIter,'rhobeg': 0.3})
        # 'rhobeg' controls the initial step size of the parameters when computing the next value of the function, default is 1.0. Smaller step increases number of iterations

    # Print loss history
    print("\nLoss History (Iteration vs. Loss):")
    for j, loss in enumerate(loss_history):
        if j%50==0 or j==mxIter-1:
            print(f"Iteration {j + 1}: Loss = {loss:.3f}")

    # Extract the optimized weights
    weightsOpt = result.x
    wMin=min(weightsOpt)
    wMax=max(weightsOpt)
    wAvr=np.mean(weightsOpt)

    print('num par=',weightsOpt.shape,'W in [%.2f,%.2f] avr=%.2f'%(wMin,wMax,wAvr))

    # Test the model
    Y_pred_test = forward_pass(X_test, weightsOpt)
    Y_pred_test_class = np.argmax(Y_pred_test, axis=1)
    Y_test_class = np.argmax(y_test, axis=1)

    # Calculate accuracy
    accuracy = np.mean(Y_pred_test_class == Y_test_class)
    print(f"Accuracy: {accuracy * 100:.2f}%")

    # Compute the confusion matrix
    from sklearn.metrics import confusion_matrix
    conf_matrix = confusion_matrix(Y_test_class,Y_pred_test_class)
    print('\nconfusion matrix, test samples:%d'%(X_test.shape[0]))
    for i,rec  in enumerate(conf_matrix):
        print('true:%d  reco:%s'%(i,rec))
